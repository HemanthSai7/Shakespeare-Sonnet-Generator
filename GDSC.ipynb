{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7GZPvXGJy76"
      },
      "source": [
        "# ML Task\n",
        "\n",
        "Hugging Face is very nice to us to include all the functionality needed for GPT2 to be used in classification tasks.\n",
        "\n",
        "**Main idea**: Since GPT2 is a decoder transformer, the last token of the input sequence is used to make predictions about the next token that should follow the input. This means that the last token of the input sequence contains all the information needed in the prediction. With this in mind we can use that information to make a prediction in a classification task instead of generation task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beaIoc7yIn14"
      },
      "source": [
        "### Install required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLt2wUWCSneb",
        "outputId": "76eed86c-3df6-440d-e082-b94c26174e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.7 MB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 49.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 73.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqv1O1EYIuDL"
      },
      "source": [
        "### Import important libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pRc6U6HJSrHh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from transformers import (GPT2Tokenizer, \n",
        "                          GPT2LMHeadModel,\n",
        "                          AdamW, \n",
        "                          get_linear_schedule_with_warmup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLXD_Ak2Iyge"
      },
      "source": [
        "### Configuration class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bW3Xk9ihSuHF"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    patience=5\n",
        "    num_workers=2\n",
        "    \n",
        "    batch_size=256\n",
        "    lr=0.001\n",
        "    epochs=10\n",
        "    load_weight_path=\"\"\n",
        "    save_file_name=\"model_weights_gpt2\"\n",
        "    MODEL_NAME=\"gpt2\"\n",
        "    \n",
        "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8ghPwc7PSvWT"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(Config.MODEL_NAME)\n",
        "    SPECIAL_TOKENS_DICT = {\n",
        "        'pad_token': '<pad>',\n",
        "    }\n",
        "    tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\n",
        "\n",
        "    model = GPT2LMHeadModel.from_pretrained(Config.MODEL_NAME)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EgJ-XIluS7qa"
      },
      "outputs": [],
      "source": [
        "class SonnetDataset(Dataset):\n",
        "    def __init__(self,sonnet_files,tokenizer):\n",
        "      self.sonnet_files=sonnet_files\n",
        "      self.tokenizer=tokenizer\n",
        "      self.eos_tok=\"<|endoftext|>\"       \n",
        "      self.sonnets=self.load_sonnets()  \n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        sonnet=self.sonnets[idx]\n",
        "        \n",
        "        inputs=self.tokenizer.encode_plus(\n",
        "            sonnet,\n",
        "            None,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=256,\n",
        "            )\n",
        "\n",
        "        ids=inputs[\"input_ids\"]\n",
        "        mask=inputs[\"attention_mask\"]\n",
        "\n",
        "\n",
        "        return {\"ids\":torch.tensor(ids,dtype=torch.long),\n",
        "                \"mask\":torch.tensor(mask,dtype=torch.long),\n",
        "                \"target\":torch.tensor(ids,dtype=torch.long)\n",
        "                }\n",
        "\n",
        "    def load_sonnets(self):\n",
        "      sonnetlist=[]\n",
        "      for sonnet_file in self.sonnet_files:\n",
        "        sonnet=open(sonnet_file,\"r\").readline()\n",
        "        sonnet = f\"Quote: {str(sonnet)} {self.eos_tok}\"\n",
        "        sonnetlist.append(sonnet)\n",
        "      return sonnetlist                \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sonnets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "x5T9m3nnS9Ic"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class Engine:\n",
        "    \n",
        "    def __init__(self, model, device, config, save_file_name = 'model_weights', weight_path='./'):\n",
        "        \n",
        "        self.train_loss=dict()\n",
        "        self.valid_loss=dict()\n",
        "        self.model=model\n",
        "        self.device=device\n",
        "        self.config=config\n",
        "        self.best_score=0\n",
        "        self.best_loss=5000\n",
        "        self.save_file_name = save_file_name\n",
        "        self.weight_path = weight_path\n",
        "\n",
        "    def fit(self, train_loader, valid_loader):\n",
        "\n",
        "      num_train_steps = int(len(train_loader) / self.config.batch_size * self.config.epochs)\n",
        "      self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.config.lr)\n",
        "      self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
        "      \n",
        "      for epoch in range(self.config.epochs):\n",
        "        \n",
        "        t=time.time()\n",
        "        print(\"Training Started...\")\n",
        "        \n",
        "        summary_loss = self.train_one_epoch(train_loader)\n",
        "        self.train_loss[epoch] = summary_loss.avg\n",
        "\n",
        "        print(f'Train : Epoch {epoch}: | Summary Loss: {summary_loss.avg} | Training time: {time.time() - t}')\n",
        "            \n",
        "        t=time.time()\n",
        "        print(\"Validation Started...\")\n",
        "        \n",
        "        summary_loss = self.validation(valid_loader)\n",
        "        self.valid_loss[epoch] = summary_loss.avg\n",
        "\n",
        "        print(f'Valid : Epoch {epoch}: | Summary Loss: {summary_loss.avg} | Training time: {time.time() - t}')\n",
        "        \n",
        "        if not self.best_score:\n",
        "            self.best_score = summary_loss.avg\n",
        "            print(f'Saving model with lowest validation loss as {self.best_score}')\n",
        "            self.model.eval()   \n",
        "            patience = self.config.patience\n",
        "            torch.save({'model_state_dict': self.model.state_dict(),'best_score': self.best_score, 'epoch': epoch},  f\"{self.weight_path}/{self.save_file_name}.pt\")\n",
        "            continue  \n",
        "\n",
        "        if summary_loss.avg <= self.best_score:\n",
        "            self.best_score = summary_loss.avg\n",
        "            patience = self.config.patience  \n",
        "            print('Improved model with lowest validation loss as {}'.format(self.best_score))\n",
        "            torch.save({'model_state_dict': self.model.state_dict(),'best_score': self.best_score, 'epoch': epoch},  f\"{self.weight_path}/{self.save_file_name}.pt\")\n",
        "        else:\n",
        "            patience -= 1\n",
        "            print('Patience Reduced')\n",
        "            if patience == 0:\n",
        "                print(f'Early stopping. Lowest validation loss achieved: {self.best_score}')\n",
        "                break\n",
        "\n",
        "    def train_one_epoch(self, train_loader):\n",
        "      self.model.train()\n",
        "\n",
        "      t = time.time()\n",
        "      summary_loss = AverageMeter()\n",
        "      \n",
        "      for steps, data in enumerate(tqdm(train_loader)):\n",
        "          ids = data[\"ids\"]\n",
        "          mask = data[\"mask\"]\n",
        "          labels = data['target']\n",
        "\n",
        "          ids = ids.to(self.device, dtype=torch.long)\n",
        "          mask = mask.to(self.device, dtype=torch.long)\n",
        "          labels = labels.to(self.device,dtype=torch.long)\n",
        "            \n",
        "          self.optimizer.zero_grad()\n",
        "          outputs = self.model(\n",
        "              input_ids =ids,\n",
        "              attention_mask=mask,\n",
        "              labels = labels\n",
        "          )\n",
        "\n",
        "          loss, logits = outputs[:2]                        \n",
        "          loss.backward()\n",
        "\n",
        "          self.optimizer.step()\n",
        "          self.scheduler.step()\n",
        "\n",
        "          summary_loss.update(loss.detach().item(), self.config.batch_size)\n",
        "\n",
        "      return summary_loss\n",
        "\n",
        "    def validation(self, valid_loader):\n",
        "      self.model.eval()\n",
        "\n",
        "      t = time.time()\n",
        "      summary_loss = AverageMeter()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for steps, data in enumerate(tqdm(valid_loader)):\n",
        "            ids = data[\"ids\"]\n",
        "            mask = data[\"mask\"]\n",
        "            labels = data['target']\n",
        "\n",
        "            ids = ids.to(self.device, dtype=torch.long)\n",
        "            mask = mask.to(self.device, dtype=torch.long)\n",
        "            labels = labels.to(self.device,dtype=torch.long)\n",
        "              \n",
        "            outputs = self.model(\n",
        "                input_ids =ids,\n",
        "                attention_mask=mask,\n",
        "                labels = labels\n",
        "            )\n",
        "\n",
        "            loss, logits = outputs[:2]  \n",
        "            summary_loss.update(loss.detach().item(), self.config.batch_size) \n",
        "      return summary_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ANpiLtMqTB84"
      },
      "outputs": [],
      "source": [
        "def perform_run(data_path,config,model,tokenizer,weight_path='./',load_weights_path=None):\n",
        "    \n",
        "    sonnet_files=[data_path]\n",
        "    datasett=SonnetDataset(sonnet_files,tokenizer)\n",
        "    indices=list(range(len(datasett)))\n",
        "    random.shuffle(indices)\n",
        "    \n",
        "    split=math.floor(0.3*len(datasett))\n",
        "    train_indices,val_indices=indices[split:],indices[:split]\n",
        "    \n",
        "    train_sampler=SubsetRandomSampler(train_indices)\n",
        "    val_sampler=SubsetRandomSampler(val_indices)\n",
        "    \n",
        "    train_loader=DataLoader(datasett,batch_size=config.batch_size,\n",
        "                           sampler=train_sampler,num_workers=config.num_workers)\n",
        "    \n",
        "    val_loader=DataLoader(datasett,batch_size=config.batch_size,\n",
        "                           sampler=train_sampler,num_workers=config.num_workers)\n",
        "    if load_weights_path is not None:\n",
        "        model.load_state_dict(torch.load(load_weights_path+f\"{config.save_file_name}.pt\")[\"model_state_dict\"])\n",
        "        print(\"Weight loaded\")\n",
        "        \n",
        "    engine=Engine(model=model.to(config.device),device=config.device,\n",
        "                config=config,save_file_name=config.save_file_name,\n",
        "                weight_path=weight_path)\n",
        "    \n",
        "    engine.fit(train_loader,val_loader)\n",
        "    \n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True   \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAqvAJdjTDow"
      },
      "outputs": [],
      "source": [
        "seed_everything(42)\n",
        "model,tokenizer=get_model()\n",
        "perform_run('Sonnets.txt',Config,model,tokenizer,Config.load_weight_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model,tokenizer=get_model()\n",
        "prompt=\"To eat the world's due, by the grave and thee.\"\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(Config.device)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated, \n",
        "                                do_sample=True,   \n",
        "                                top_k=50, \n",
        "                                max_length = 300,\n",
        "                                top_p=0.95, \n",
        "                                num_return_sequences=3\n",
        "                                )\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(f\"{i}:\\n\\n{tokenizer.decode(sample_output, skip_special_tokens=True)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GDSC.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
