{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel,get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader,Dataset,SubsetRandomSampler\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "  datadir=\"../data\"\n",
    "  patience = 5\n",
    "\n",
    "  batch_size = 32\n",
    "  num_workers = 4\n",
    "\n",
    "  lr = 0.00003\n",
    "  n_epoches = 100\n",
    "  load_weights_path = \"model/\"\n",
    "  save_file_name = \"model_weights_gpt2\"\n",
    "  MODEL_NAME = \"gpt2\"\n",
    "\n",
    "  huggingFace_model = \"model/huggingFace\"\n",
    "  huggingFace_tokenizer = \"model/huggingFace\"\n",
    "\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    tokenizer=GPT2Tokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    SPECIAL_TOKENS_DICT={\n",
    "        'pad_token': '<PAD>',\n",
    "    }\n",
    "    tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\n",
    "\n",
    "    model=GPT2LMHeadModel.from_pretrained(Config.MODEL_NAME)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SonnetDataset(Dataset):\n",
    "    def __init__(self,tokenizer):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.eos_tok=\"<|endoftext|>\"       \n",
    "        self.sonnets=[] \n",
    "\n",
    "        with open('../data/Sonnets.txt') as txt_file:\n",
    "          sonnett=txt_file.lower().readlines()\n",
    "\n",
    "          for line in sonnett:\n",
    "            sonnet=f\"Sonnet: {str(line)}{self.eos_tok}\"\n",
    "            self.sonnets.append(sonnet)\n",
    "\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        sonnet=self.sonnets[idx]\n",
    "        \n",
    "        inputs=self.tokenizer.encode_plus(\n",
    "            sonnet,\n",
    "            None,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=256,\n",
    "            )\n",
    "\n",
    "        ids=inputs[\"input_ids\"]\n",
    "        mask=inputs[\"attention_mask\"]\n",
    "\n",
    "\n",
    "        return {\"ids\":torch.tensor(ids,dtype=torch.long),\n",
    "                \"mask\":torch.tensor(mask,dtype=torch.long),\n",
    "                \"target\":torch.tensor(ids,dtype=torch.long)}   \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sonnets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True   \n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "class Engine:\n",
    "    \n",
    "    def __init__(self, model, device, config, save_file_name = 'model_weights', weight_path='./'):\n",
    "        \n",
    "        self.train_loss=dict()\n",
    "        self.valid_loss=dict()\n",
    "        self.model=model\n",
    "        self.device=device\n",
    "        self.config=config\n",
    "        self.best_score=0\n",
    "        self.best_loss=5000\n",
    "        self.save_file_name = save_file_name\n",
    "        self.weight_path = weight_path\n",
    "\n",
    "    def fit(self, train_loader, valid_loader):\n",
    "\n",
    "      num_train_steps = int(len(train_loader) / self.config.batch_size * self.config.epochs)\n",
    "      self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.config.lr)\n",
    "      self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "      \n",
    "      for epoch in range(self.config.epochs):\n",
    "        \n",
    "        t=time.time()\n",
    "        print(\"Training Started...\")\n",
    "        \n",
    "        summary_loss = self.train_one_epoch(train_loader)\n",
    "        self.train_loss[epoch] = summary_loss.avg\n",
    "\n",
    "        print(f'Train : Epoch {epoch}: | Summary Loss: {summary_loss.avg} | Training time: {time.time() - t}')\n",
    "            \n",
    "        t=time.time()\n",
    "        print(\"Validation Started...\")\n",
    "        \n",
    "        summary_loss = self.validation(valid_loader)\n",
    "        self.valid_loss[epoch] = summary_loss.avg\n",
    "\n",
    "        print(f'Valid : Epoch {epoch}: | Summary Loss: {summary_loss.avg} | Training time: {time.time() - t}')\n",
    "        \n",
    "        if not self.best_score:\n",
    "            self.best_score = summary_loss.avg\n",
    "            print(f'Saving model with lowest validation loss as {self.best_score}')\n",
    "            self.model.eval()   \n",
    "            patience = self.config.patience\n",
    "            torch.save({'model_state_dict': self.model.state_dict(),'best_score': self.best_score, 'epoch': epoch},  f\"{self.weight_path}/{self.save_file_name}.pt\")\n",
    "            continue  \n",
    "\n",
    "        if summary_loss.avg <= self.best_score:\n",
    "            self.best_score = summary_loss.avg\n",
    "            patience = self.config.patience  \n",
    "            print('Improved model with lowest validation loss as {}'.format(self.best_score))\n",
    "            torch.save({'model_state_dict': self.model.state_dict(),'best_score': self.best_score, 'epoch': epoch},  f\"{self.weight_path}/{self.save_file_name}.pt\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            print('Patience Reduced')\n",
    "            if patience == 0:\n",
    "                print(f'Early stopping. Lowest validation loss achieved: {self.best_score}')\n",
    "                break\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "      self.model.train()\n",
    "\n",
    "      t = time.time()\n",
    "      summary_loss = AverageMeter()\n",
    "      \n",
    "      for steps, data in enumerate(tqdm(train_loader)):\n",
    "          ids = data[\"ids\"]\n",
    "          mask = data[\"mask\"]\n",
    "          labels = data['target']\n",
    "\n",
    "          ids = ids.to(self.device, dtype=torch.long)\n",
    "          mask = mask.to(self.device, dtype=torch.long)\n",
    "          labels = labels.to(self.device,dtype=torch.long)\n",
    "            \n",
    "          self.optimizer.zero_grad()\n",
    "          outputs = self.model(\n",
    "              input_ids =ids,\n",
    "              attention_mask=mask,\n",
    "              labels = labels\n",
    "          )\n",
    "\n",
    "          loss, logits = outputs[:2]                        \n",
    "          loss.backward()\n",
    "\n",
    "          self.optimizer.step()\n",
    "          self.scheduler.step()\n",
    "\n",
    "          summary_loss.update(loss.detach().item(), self.config.batch_size)\n",
    "\n",
    "      return summary_loss\n",
    "\n",
    "    def validation(self, valid_loader):\n",
    "      self.model.eval()\n",
    "\n",
    "      t = time.time()\n",
    "      summary_loss = AverageMeter()\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for steps, data in enumerate(tqdm(valid_loader)):\n",
    "            ids = data[\"ids\"]\n",
    "            mask = data[\"mask\"]\n",
    "            labels = data['target']\n",
    "\n",
    "            ids = ids.to(self.device, dtype=torch.long)\n",
    "            mask = mask.to(self.device, dtype=torch.long)\n",
    "            labels = labels.to(self.device,dtype=torch.long)\n",
    "              \n",
    "            outputs = self.model(\n",
    "                input_ids =ids,\n",
    "                attention_mask=mask,\n",
    "                labels = labels\n",
    "            )\n",
    "\n",
    "            loss, logits = outputs[:2]  \n",
    "            summary_loss.update(loss.detach().item(), self.config.batch_size) \n",
    "      return summary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_run(data_path,config,model,tokenizer,weight_path='./',load_weights_path=None):\n",
    "    \n",
    "    sonnet_files=[data_path]\n",
    "    datasett=SonnetDataset(sonnet_files,tokenizer)\n",
    "    indices=list(range(len(datasett)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    split=math.floor(0.3*len(datasett))\n",
    "    train_indices,val_indices=indices[split:],indices[:split]\n",
    "    \n",
    "    train_sampler=SubsetRandomSampler(train_indices)\n",
    "    val_sampler=SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    train_loader=DataLoader(datasett,batch_size=config.batch_size,\n",
    "                           sampler=train_sampler,num_workers=config.num_workers)\n",
    "    \n",
    "    val_loader=DataLoader(datasett,batch_size=config.batch_size,\n",
    "                           sampler=train_sampler,num_workers=config.num_workers)\n",
    "    if load_weights_path is not None:\n",
    "        model.load_state_dict(torch.load(load_weights_path+f\"{config.save_file_name}.pt\")[\"model_state_dict\"])\n",
    "        print(\"Weight loaded\")\n",
    "        \n",
    "    engine=Engine(model=model.to(config.device),device=config.device,\n",
    "                config=config,save_file_name=config.save_file_name,\n",
    "                weight_path=weight_path)\n",
    "    \n",
    "    engine.fit(train_loader,val_loader)\n",
    "    \n",
    "seed_everything(42)\n",
    "model,tokenizer=get_model()\n",
    "perform_run('Sonnets.txt',Config,model,tokenizer,Config.load_weight_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorchML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f644ea3d0090735a2d7e1c5a44574a22cce71e235e391c5390fef51a9d413ba8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
